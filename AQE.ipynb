{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2gz6i3pYBLH",
        "outputId": "1435eae8-029e-43a3-8416-cc89f177f412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Sort ['state ASC NULLS FIRST], true\n",
            "+- Aggregate [state#6], [state#6, sum(amount#1) AS sum(amount)#19]\n",
            "   +- Project [store_id#0L, amount#1, store_name#5, state#6]\n",
            "      +- Join Inner, (store_id#0L = store_id#4L)\n",
            "         :- LogicalRDD [store_id#0L, amount#1], false\n",
            "         +- LogicalRDD [store_id#4L, store_name#5, state#6], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "state: string, sum(amount): double\n",
            "Sort [state#6 ASC NULLS FIRST], true\n",
            "+- Aggregate [state#6], [state#6, sum(amount#1) AS sum(amount)#19]\n",
            "   +- Project [store_id#0L, amount#1, store_name#5, state#6]\n",
            "      +- Join Inner, (store_id#0L = store_id#4L)\n",
            "         :- LogicalRDD [store_id#0L, amount#1], false\n",
            "         +- LogicalRDD [store_id#4L, store_name#5, state#6], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Sort [state#6 ASC NULLS FIRST], true\n",
            "+- Aggregate [state#6], [state#6, sum(amount#1) AS sum(amount)#19]\n",
            "   +- Project [amount#1, state#6]\n",
            "      +- Join Inner, (store_id#0L = store_id#4L)\n",
            "         :- Filter isnotnull(store_id#0L)\n",
            "         :  +- LogicalRDD [store_id#0L, amount#1], false\n",
            "         +- Project [store_id#4L, state#6]\n",
            "            +- Filter isnotnull(store_id#4L)\n",
            "               +- LogicalRDD [store_id#4L, store_name#5, state#6], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Sort [state#6 ASC NULLS FIRST], true, 0\n",
            "   +- Exchange rangepartitioning(state#6 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=51]\n",
            "      +- HashAggregate(keys=[state#6], functions=[sum(amount#1)], output=[state#6, sum(amount)#19])\n",
            "         +- Exchange hashpartitioning(state#6, 200), ENSURE_REQUIREMENTS, [plan_id=48]\n",
            "            +- HashAggregate(keys=[state#6], functions=[partial_sum(amount#1)], output=[state#6, sum#23])\n",
            "               +- Project [amount#1, state#6]\n",
            "                  +- SortMergeJoin [store_id#0L], [store_id#4L], Inner\n",
            "                     :- Sort [store_id#0L ASC NULLS FIRST], false, 0\n",
            "                     :  +- Exchange hashpartitioning(store_id#0L, 200), ENSURE_REQUIREMENTS, [plan_id=40]\n",
            "                     :     +- Filter isnotnull(store_id#0L)\n",
            "                     :        +- Scan ExistingRDD[store_id#0L,amount#1]\n",
            "                     +- Sort [store_id#4L ASC NULLS FIRST], false, 0\n",
            "                        +- Exchange hashpartitioning(store_id#4L, 200), ENSURE_REQUIREMENTS, [plan_id=41]\n",
            "                           +- Project [store_id#4L, state#6]\n",
            "                              +- Filter isnotnull(store_id#4L)\n",
            "                                 +- Scan ExistingRDD[store_id#4L,store_name#5,state#6]\n",
            "\n",
            "+-----+--------------------+\n",
            "|state|         sum(amount)|\n",
            "+-----+--------------------+\n",
            "|   CA|1.2135061069516069E8|\n",
            "|   FL| 1.466120441977841E8|\n",
            "|   NY|1.0093535091556348E8|\n",
            "|   TX| 2.662809716725092E9|\n",
            "+-----+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import random\n",
        "\n",
        "# Create Spark Session with AQE enabled\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"AQE Demo\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create sample datasets\n",
        "# Sales data with skewed distribution\n",
        "def create_sales_data():\n",
        "    data = []\n",
        "    # Create normal distribution\n",
        "    for i in range(1000000):\n",
        "        store_id = random.randint(1, 100)\n",
        "        amount = random.uniform(10, 1000)\n",
        "        data.append((store_id, amount))\n",
        "\n",
        "    # Add skewed data for store_id 1\n",
        "    for i in range(5000000):\n",
        "        data.append((1, random.uniform(10, 1000)))\n",
        "\n",
        "    return spark.createDataFrame(data, [\"store_id\", \"amount\"])\n",
        "\n",
        "# Store information\n",
        "def create_store_data():\n",
        "    data = [(i, f\"Store_{i}\", random.choice([\"NY\", \"CA\", \"TX\", \"FL\"]))\n",
        "            for i in range(1, 101)]\n",
        "    return spark.createDataFrame(data, [\"store_id\", \"store_name\", \"state\"])\n",
        "\n",
        "# Create our datasets\n",
        "sales_df = create_sales_data()\n",
        "stores_df = create_store_data()\n",
        "\n",
        "# Execute a query that will benefit from AQE\n",
        "result = sales_df.join(stores_df, \"store_id\") \\\n",
        "    .groupBy(\"state\") \\\n",
        "    .agg({\"amount\": \"sum\"}) \\\n",
        "    .orderBy(\"state\")\n",
        "\n",
        "# Force execution and show results\n",
        "result.explain(True)  # Shows the adaptive plan\n",
        "result.show()\n"
      ]
    }
  ]
}